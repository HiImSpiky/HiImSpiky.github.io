---
layout: post
title: Installment 02 - Generative Adversarial Network
---

##### Introduction

This is our second installment for the project on conditional 
image generation. In this post, we present new architectures that 
achieved much better reconstruction and run several experiments 
to test the effect of captions on the generated image.

The proposed models are variants of the generative adversarial 
network, we start with a vanilla Deep Convolutional GAN (DCGAN) [1]
and then introduce the new Boundary Equilibrium GAN (BEGAN) 
architecture [2].

We also gave a shot at Wasserstein GAN (WGAN) [4], but unlike [Louis Henri Frank's](https://github.com/louishenrifranc/ImageFilling), our results were quite deceiving, had time not
been an issue it would have been interesting to take a look at the [Improved training of Wasserstein GANs paper](https://arxiv.org/pdf/1704.00028.pdf) [5], but implementing it in Theano proved quite hard.

##### Implementation Detail

The first model I explored is a standard generative adversarial 
network taken from [Philip Paquette's blog](https://github.com/ppaquette/ift-6266-project/),
the following figure illustrates it's architecture.

![_config.yml]({{ site.baseurl }}/images/gan_architecture.png)

<sup>**Fig. 1. Visual representation of the GAN framework. Code is 
available [here](https://github.com/charlesashby/conditional-image-generation).**</sup>

The generator is composed of an encoder which is a smaller version
of the SqueezeNet architecture [3] and a decoder made up of multiple
strided transposed convolutions while the discriminator is a simple
encoder.

Both the generator and discriminator were trained simultaneously for 100
epochs, the discriminator used stochastic gradient descent and the generator
used Adam, dropout was added at the middle of training and we also used 
feature matching which really helped the generator learn better
reconstruction features.

##### Data Augmentation

We did not perform much data augmentation, we simply randomly rotate the images once in a while,
from our "experiments", it is not clear at all if this trick helped the model generate better images. 

##### Caption Encoding

We decided to encode the captions using a pretrained skip-thought model [6], this network learns,
in an unsupervised way, a vector representation that preserves the syntactic and semantic properties
of the sentences it encodes.

##### Boundary Equilibrium Generative Adversarial Network
In the BEGAN framework, the discriminator is an autoencoder. The objective function is 
a minimax game but,unlike standard GANs, the generator tries to minimize the reconstruction
error of the images it provides to the discriminator while the discriminator wants 
to simultaneously maximize this error and minimize the reconstruction error of the true 
images. The following figure gives a summary of this architecture.


![_config.yml]({{ site.baseurl }}/images/began_architecture.png)
<sup>**Fig. 2. Visual representation of the BEGAN framework. Code is 
available [here](https://github.com/charlesashby/conditional-image-generation).**</sup>


The BEGAN model is based on the assumption that the reconstruction error of an 
autoencoder follows a normal distribution, when it is the case we can compute the
Wasserstein distance between them relatively easily.

Let $ L(v) = |v - D(v)| $ be the reconstruction error of some image $ v $, if $ L(X) \sim N(m_1, C_1) $ 
and $ L(G(z)) \sim N(m_2, C_2) $ then the Wasserstein distance between the true image reconstruction 
and the fake image reconstruction is given by

$$
W(L(X), L(G(z))^2 = \parallel m_1 - m_2 \parallel^2 + (C_1 + C_2 - 2\sqrt{C_1 C_2}) 
$$

Therefore, if $ \dfrac{C_1 + C_2 - 2 \sqrt{C_1 C_2}}{\parallel m_1 - m_2 \parallel^2} $ is constant or
monotonically increasing $ W(L(X), L(G(z))^2 \propto \parallel m_1 - m_2 \parallel^2 $, thus, we can 
maximize this equation by minimizing $m_1$ (which is equivalent to autoencoding the real images) and 
maximizing $m_2$.

The BEGAN objective is then given by:


$$
\left\{
    \begin{array}{ll}
        L_D = L(X) - L(G(z))  \\
        L_G = L(G(z)) 
    \end{array}
\right.

$$


Which is almost the same as the regular GAN (and WGAN), note that we don't need 
the Lipschitz conditions like in Wasserstein GAN [4],
therefore, we can train the generator and discriminator simultaneously,
we recover the ability to use optimizers with momentum and we don't have to 
clip the weights.

Training a BEGAN resulted in many failed attempts so I decided to compile a small
list of what worked best for me:

- No batchnorm in the encoders
- No ReLU/Leaky ReLU activation, exponential linear unit (ELU) seemed to work better
- No activation function in the decoders output; instead clip the values between [-1, 1]
- Do not use an autoencoder with too big capacity (e.g. SqueezeNet) for the discriminator
- Penalizing the generator with the L2 distance between the generated samples and the true
 images really helped stabilize training but resulted in square artefacts. (This one significantly helped
 when both the generator and discriminator had similar capacity, but not so much when the generator was stronger.)


##### Results


##### Experiments


