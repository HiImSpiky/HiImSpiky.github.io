---
layout: post
title: Installment 02 - Generative Adversarial Network
---

##### UNDER CONSTRUCTION!

You can find the code for my GAN [here](https://github.com/charlesashby/conditional-image-generation).

![_config.yml]({{ site.baseurl }}/images/gan_grid_1.png)

#### Introduction

This is our second installment for the project on conditional 
image generation. In this post, we present new architectures that 
achieved much better reconstruction and run several experiments 
to test the effect of captions on the generated image.

The proposed models are variants of the generative adversarial 
network, we start with a vanilla Deep Convolutional GAN (DCGAN) [1]
and then introduce the new Boundary Equilibrium GAN (BEGAN) 
architecture [2].

#### Implementation Detail

The first model I explored is a standard generative adversarial 
network taken from [Philip Paquette's blog](https://github.com/ppaquette/ift-6266-project/),
the following figure illustrates it's architecture.

![_config.yml]({{ site.baseurl }}/images/gan_architecture.png)

The generator is composed of an encoder which is a smaller version
of the SqueezeNet architecture [3] and a decoder made up of multiple
strided transposed convolutions while the discriminator is a simple
encoder.

Both the generator and discriminator were trained simultaneously for 100
epochs, the discriminator used stochastic gradient descent and the generator
used Adam, dropout was added at the middle of training and we also used 
feature matching which really helped the generator learn better
reconstruction features.

In the BEGAN framework, the discriminator is an autoencoder and the objective function
is a minimax game but, unlike standard GANs, the generator tries to minimize the 
reconstruction error of the image it provides to the discriminator while
the discriminator wants to simultaneously maximize this error and minimize the
reconstruction error of the true images. The following figure gives a summary
of this architecture.

The BEGAN model is based on the assumption that the reconstruction error of an 
autoencoder follows a normal distribution, when it is the case we can compute the
Wasserstein distance between them relatively easily:

$$
L(v) = |v - D(v)|
$$

Note that we don't need the Lipschitz conditions like in Wasserstein GAN [4],
therefore, we can train the generator and discriminator simultaneously,
we recover the ability to use optimizers with momentum and we don't have to 
clip the weights.





Training a BEGAN resulted in many failed attempts and I've compiled a small
list of what worked best for me:

- No batchnorm in the encoder
- No ReLu activation, exponential linear unit (elu) seemed to work better
- No activation function in the decoder output; instead clip the values between [-1, 1]
- Penalizing the generator with the L2 distance between the generated samples and the true
 images really helped stabilize training but resulted in square artefacts.
- Do not use an autoencoder with too big capacity (e.g. SqueezeNet) for the discriminator
